# Test Configuration for PCC Data Pipeline - Reduced Sample Size

# Data Sources
source:
  type: "synthetic"  # synthetic, csv, parquet, kafka, bigquery
  path: null  # for file-based sources
  kafka:
    bootstrap_servers: ["localhost:9092"]
    topic: "privacy-intent-data"
    group_id: "data-pipeline-consumer"
  bigquery:
    project_id: "your-project-id"
    dataset: "privacy_intent"
    table: "raw_data"

# Processing Configuration
processing:
  engine: "pandas"  # spark, beam, ray, pandas
  distributed:
    enabled: false
    num_workers: 4
    memory_per_worker: "2g"
  streaming:
    enabled: false
    checkpoint_location: "checkpoints/"
    watermark_delay: "1 minute"

# Enhanced Features Configuration
features:
  # Toggle for text feature engineering
  engineer_text_features: true

  # Configuration for text features
  text_features:
    privacy_keywords: true
    sentiment_analysis: true
    linguistic_features: true
    statistical_features: true

# Embeddings Configuration
embeddings:
  generate_embeddings: true
  embedding_type: "privacy_domain"  # sentence_transformer, tfidf, privacy_domain
  models:
    sentence_transformer:
      model_name: "all-MiniLM-L6-v2"
      batch_size: 32
    tfidf:
      max_features: 5000
      ngram_range: [1, 2]
    cache_dir: "cache/embeddings"

# Enhanced Synthetic Data Configuration
synthetic_data:
  variation_level: 0.3
  intent_distribution:
    privacy_request: 0.25
    data_deletion: 0.20
    opt_out: 0.25
    other: 0.30
  time_patterns:
    business_hours: [9, 17]
    weekdays_only: true
    seasonal_variation: true

# Data Validation
validation:
  schema:
    - column: "text"
      type: "string"
      nullable: false
    - column: "intent"
      type: "string"
      nullable: false
      allowed_values: ["privacy_request", "data_deletion", "opt_out", "other"]
    - column: "confidence"
      type: "float"
      nullable: false
      min_value: 0.0
      max_value: 1.0
    - column: "timestamp"
      type: "datetime"
      nullable: false
  
  quality_checks:
    - check_type: "completeness"
      threshold: 0.95
    - check_type: "uniqueness"
      threshold: 0.99
    - check_type: "validity"
      threshold: 0.98
    - check_type: "consistency"
      threshold: 0.95

# Sampling & Balancing - REDUCED FOR TESTING
sampling:
  strategy: "stratified"
  by: "intent"
  n: 100  # Reduced from 10000 to 100 for faster testing
  balance_classes: true
  oversampling_method: "smote"

# Partitioning
partitioning:
  enabled: true
  strategy: "date"  # date, hash, range
  column: "timestamp"
  format: "YYYY-MM-DD"

# Output Configuration
output:
  format: "parquet"  # csv, parquet, bigquery
  path: "output/curated_training_data_test.parquet"
  compression: "snappy"
  bigquery:
    project_id: "your-project-id"
    dataset: "privacy_intent"
    table: "curated_data"
    write_disposition: "WRITE_TRUNCATE"

# Lineage Tracking
lineage:
  enabled: true
  storage_dir: "metadata/lineage"
  track_datasets: true
  track_stages: true
  track_metrics: true

# Pipeline Contracts
contracts:
  enabled: true
  validation_enabled: true
  contracts_dir: "contracts"
  export_documentation: true

# Monitoring & Logging
monitoring:
  log_level: "INFO"
  metrics_enabled: true
  mlflow:
    tracking_uri: "http://localhost:5000"
    experiment_name: "privacy-intent-pipeline"
  
# Performance Tuning
performance:
  batch_size: 1000  # Reduced from 10000
  cache_intermediates: true
  optimize_joins: true

# Version and Metadata
version: "2.0.0"
pipeline_name: "privacy-intent-data-pipeline"
description: "Enhanced enterprise data pipeline for privacy intent classification - TEST CONFIG" 